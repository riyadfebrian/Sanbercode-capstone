# -*- coding: utf-8 -*-
"""TugasPythonLanjutan_Hari_8_Riyad Febrian.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uh5KqudtNSIdQy6W_AAFp28m1fQb3RCU

## Import Library
"""

!pip install Sastrawi

import tweepy
import string
import nltk
import re
import requests
import pandas as pd
import numpy as np
from nltk.corpus import stopwords
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

nltk.download("stopwords")
nltk.download('punkt')

"""## API Credential"""

# Declare API Key
consumer_key = "------CONSUMER KEY-------"        
consumer_secret = "------------CONSUMER SECRET KEY-------------"

# konfigurasi consumer_key, consumer_secret
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
# authentication
api = tweepy.API(auth)

"""## Stemmer & Stopwords Configuration"""

# Stemmer Object
  factory = StemmerFactory()
  stemmer = factory.create_stemmer()

  # Configure Stopwords
  stop_words =  set(stopwords.words('indonesian'))

"""## Scrape Tweet

### Clean Tweet data
"""

def clean_tweets(tweet):
  # 1. Case Folding
  tweet = re.sub(r"http\S+", "", tweet) # Remove url
  tweet = re.sub(r"\d+", "", tweet) # Remove Numbers
  tweet = tweet.lower().strip() # remove trailing spaces & lowerize
  tweet = tweet.replace('\n\n', '') # Remove tweet line break

  # 2. Stemming
  tweet = stemmer.stem(tweet)

  # 3. Punctuation
  tweet = tweet.translate(str.maketrans("","",string.punctuation))

  # 4. Tokenization
  tokens = nltk.tokenize.word_tokenize(tweet)

  # 5. Remove Stop Words
  tweet = [w for w in tokens if not w in stop_words]

  return " ".join(tweet)

"""### Contoh Perhitungan Sentiment-Analysis Sanbercode"""

# search_words = "covid"
# date_since = "2020-07-23"
# new_search = search_words + " -filter:retweets"

# tweets = tweepy.Cursor(api.search,
#         q=new_search,
#         lang="in",
#         since=date_since).items(10)

# items = []
# for tweet in tweets:
#   item = []
#   item.append (' '.join(re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)", " ", tweet.text).split()))
#   items.append(item)
# hasil = pd.DataFrame(data=items, columns=['tweet'])

# hasil

pos_kata = requests.get('https://blog.sanbercode.com/wp-content/uploads/2020/07/kata_positif.txt').text
neg_kata = requests.get('https://blog.sanbercode.com/wp-content/uploads/2020/07/kata_negatif.txt').text

# pos_kata.split('\r\n')

# result = []
# for item in items:
#   count_p = 0
#   count_n = 0
  
#   for kata_pos in pos_kata.split('\r\n'):
#     if kata_pos in item[0]:
#       count_p +=1
  
#   for kata_neg in neg_kata.split('\r\n'):
#     if kata_neg in item[0]:
#       count_n +=1
#   # print ("positif: "+str(count_p))
#   # print ("negatif: "+str(count_n))

#   result.append(count_p - count_n)
#         # print ("-----------------------------------------------------")
# hasil["value"] = result
# print ("Nilai rata-rata: "+str(np.mean(hasil["value"])))
# print ("Standar deviasi: "+str(np.std(hasil["value"])))

# import matplotlib.pyplot as plt
# labels, counts = np.unique(hasil["value"], return_counts=True)
# plt.bar(labels, counts, align='center')
# plt.gca().set_xticks(labels)
# plt.show()



"""### Trending Topic"""

def scrape_topic(search_words, date_since = "2020-07-27"):
  new_search = search_words + " -filter:retweets"
  
  tweets = tweepy.Cursor(api.search,
                         q=new_search,
                         lang="in",
                         tweet_mode='extended',
                         since=date_since).items(1000)
  items = []
  for tweet in tweets:
    items.append([clean_tweets(tweet.full_text), tweet.created_at])
  
  hasil = pd.DataFrame(items, columns=['tweet', 'created_at'])

  return hasil

scrape_anies = scrape_topic(search_words="anies baswedan")
scrape_terawan = scrape_topic(search_words="terawan")
scrape_jouska = scrape_topic(search_words="jouska")

scrape_anies.to_csv('anies_tweet.csv', index=False)
scrape_terawan.to_csv('terawan_tweet.csv', index=False)
scrape_jouska.to_csv('jouska_tweet.csv', index=False)

"""## BERT Sentiment Analysis

Resource dari Mas Muhammad Fadhli

### Import Torch + Settings CUDA
"""

import torch

if torch.cuda.is_available():
  device = torch.device('cuda')

  print('there are %d GPU(s) available.' % torch.cuda.device_count())

  print('we will use the GPU: ', torch.cuda.get_device_name(0))

else:
  print("No GPU available, using the CPU instead")
  device = torch.device("cpu")

# Install Huggingface Transformer
!pip install transformers

"""### Indonesian Sentiment-Analysis Training Dataset"""

df = pd.read_csv("https://raw.githubusercontent.com/ridife/dataset-idsa/master/Indonesian%20Sentiment%20Twitter%20Dataset%20Labeled.csv", delimiter='\t')
df.shape

"""#### Stemming Data Training

Berdasarkan Paper Dataset Indonesia untuk Analisis Sentimen (JNTETI, Vol. 8, No. 4, November 2019 ), Data Training telah melalui 3 proses yaitu pembersihan elemen pengganggu, stemming, dan penghilangan stopwords.

Namun, ternyata dataset label masih belum di stemming, sehingga stemming menggunakan library Sastrawi akan kita gunakan
"""

df['Tweet'] = df['Tweet'].apply(lambda x: stemmer.stem(x))

df.to_csv('indo-sentiment-analysis-sentiment.csv', index=False)

"""#### Add Lexical Positif Negatif Words"""

add_positif = []
for i in pos_kata.split('\r\n'):
  add_positif.append([1, stemmer.stem(i)])

add_negatif = []
for i in neg_kata.split('\r\n'):
  add_negatif.append([2, stemmer.stem(i)])

df_pos = pd.DataFrame(add_positif, columns=['sentimen', 'Tweet'])
df_neg = pd.DataFrame(add_negatif, columns=['sentimen', 'Tweet'])

df_posneg = df_pos.append(df_neg, ignore_index = True)

df = df.append(df_posneg, ignore_index=True)

df.shape

"""#### Lazy Ways"""

# df = pd.read_csv('indo-sentiment-analysis-sentiment.csv')
# df.shape

"""#### Transform list"""

# Ubah value sentimen negatif menjadi 2
df['sentimen'] = df['sentimen'].replace(-1, 2)

sentences = df.Tweet.values
labels = df.sentimen.values

"""### Load BERT Tokenizer"""

from transformers import BertTokenizer

print("Loading BERT Tokenizer")
tokenizer = BertTokenizer.from_pretrained('cahya/bert-base-indonesian-522M', do_lower_case=True)

input_ids = []

for sent in sentences:
  encoded_sent = tokenizer.encode(
      sent,
      add_special_tokens = True
  )
  input_ids.append(encoded_sent)

print("Max sentence length: ", max([len(sen) for sen in input_ids]))

from keras.preprocessing.sequence import pad_sequences

MAX_LEN = 64

print("Padding/truncating all sentences to %d values" % MAX_LEN)
print('Padding token: "{:}", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))

input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype='long', value=0, truncating='post', padding='post')

print("Done")

attention_mask = []

for sent in input_ids:
  att_mask = [int(token_id > 0) for token_id in sent]

  attention_mask.append(att_mask)

"""### Data Preparation"""

from sklearn.model_selection import train_test_split

train_input, test_input, train_labels, test_labels = train_test_split(input_ids,
                                                                      labels,
                                                                      random_state=123,
                                                                      test_size=0.1)
train_mask, test_mask, _, _ = train_test_split(attention_mask,
                                               labels,
                                               random_state=123,
                                               test_size=0.1)

train_input, validation_input, train_labels, validation_labels = train_test_split(train_input,
                                                                                  train_labels,
                                                                                  random_state=246,
                                                                                  test_size=0.15)
train_mask, validation_mask, _, _ = train_test_split(train_mask,
                                                     train_mask,
                                                     random_state=246,
                                                     test_size=0.15)

import numpy as np
print("== Train ==")
print("Input: ", train_input.shape)
print("Label: ", train_labels.shape)
print("Mask: ", np.array(train_mask).shape)

print("\n== Validation ==")
print("Input: ", validation_input.shape)
print("Label: ", validation_labels.shape)
print("Mask: ", np.array(validation_mask).shape)

print("\n== Test ==")
print("Input: ", test_input.shape)
print("Label: ", test_labels.shape)
print("Mask: ", np.array(test_mask).shape)

train_input = torch.tensor(train_input)
train_labels = torch.tensor(train_labels)
train_mask = torch.tensor(train_mask)

validation_input = torch.tensor(validation_input)
validation_labels = torch.tensor(validation_labels)
validation_mask = torch.tensor(validation_mask)

test_input = torch.tensor(test_input)
test_labels = torch.tensor(test_labels)
test_mask = torch.tensor(test_mask)

from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler

batch_size = 32

train_data = TensorDataset(train_input, train_mask, train_labels)
train_sampler = RandomSampler(train_data)
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)

validation_data = TensorDataset(validation_input, validation_mask, validation_labels)
validation_sampler = SequentialSampler(validation_data)
validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)

test_data = TensorDataset(test_input, test_mask, test_labels)
test_sampler = SequentialSampler(test_data)
test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)

"""### BERT Model Preparation"""

from transformers import BertForSequenceClassification, AdamW, BertConfig

model = BertForSequenceClassification.from_pretrained(
    "cahya/bert-base-indonesian-522M",
    num_labels = 3,
    output_attentions = False,
    output_hidden_states = False
)

model.cuda()

params = list(model.named_parameters())

print("The BERT model has {:} different named parameters.".format(len(params)))

print("==== Embedding Layer ====")
for p in params[0:5]:
  print("{:<60} {:>12}".format(p[0], str(tuple(p[1].size()))))

print("==== First Transformers ====")
for p in params[5:21]:
  print("{:<60} {:>12}".format(p[0], str(tuple(p[1].size()))))

print("==== Output Layer ====")
for p in params[-4:]:
  print("{:<60} {:>12}".format(p[0], str(tuple(p[1].size()))))

optimizer = AdamW(
    model.parameters(),
    lr = 2e-5,
    eps = 1e-8
)

from transformers import get_linear_schedule_with_warmup

epochs = 4

total_steps = len(train_dataloader) * epochs

scheduler = get_linear_schedule_with_warmup(optimizer,
                                             num_warmup_steps = 0,
                                             num_training_steps = total_steps)

import numpy as np

def flat_accuracy(preds, labels):
  pred_flat = np.argmax(preds, axis=1).flatten()
  labels_flat = labels.flatten()
  return np.sum(pred_flat == labels_flat) / len(labels_flat)

import time
import datetime

def format_time(elapsed):
  elapsed_rounded = int(round(elapsed))
  return str(datetime.timedelta(seconds=elapsed_rounded))

"""### BERT Training"""

import random

seed_val = 42

random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)

loss_values = []

for epoch_i in range(0, epochs):

  # ===================================
  #              Training
  # ===================================

  print("======= Epoch {:} / {:} =======".format(epoch_i+1, epochs))
  print("Training...")

  t0 = time.time()

  total_loss = 0

  model.train()

  # For each batch of training data
  for step, batch in enumerate(train_dataloader):
    
    # Progress update every 40 batches
    if step % 40 == 0 and not step == 0:
      elapsed = format_time(time.time() - t0)

      print("Batch {:>5,} of {:>5,}.     Elapsed: {:}".format(step, len(train_dataloader), elapsed))
    
    b_input_ids = batch[0].to(device)
    b_input_mask = batch[1].to(device)
    b_labels = batch[2].to(device)

    model.zero_grad()

    outputs = model(b_input_ids,
                    token_type_ids=None,
                    attention_mask=b_input_mask,
                    labels=b_labels)
    
    loss = outputs[0]

    total_loss += loss.item()

    loss.backward()

    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

    optimizer.step()

    scheduler.step()

  avg_train_loss = total_loss / len(train_dataloader)

  loss_values.append(avg_train_loss)

  print("   Average training loss: {0:.2f}".format(avg_train_loss))
  print("   Training epoch took: {:}".format(format_time(time.time() - t0)))

  # ===================================
  #             Validation
  # ===================================

  print("Running Validation...")

  t0 = time.time()

  model.eval()

  eval_loss, eval_accuracy = 0, 0
  nb_eval_steps, nb_eval_examples = 0, 0

  for batch in validation_dataloader:

    batch = tuple(t.to(device) for t in batch)

    b_input_ids, b_input_mask, b_labels = batch

    with torch.no_grad():
      outputs = model(b_input_ids,
                      token_type_ids=None,
                      attention_mask=b_input_mask)
    
    logits = outputs[0]
    logits = logits.detach().cpu().numpy()
    label_ids = b_labels.to('cpu').numpy()

    tmp_eval_accuracy = flat_accuracy(logits, label_ids)

    eval_accuracy += tmp_eval_accuracy

    nb_eval_steps += 1
  
  print("   Accuracy: {0:.2f}".format(eval_accuracy/nb_eval_steps))
  print("   Validation took: {:}".format(format_time(time.time() - t0)))

print("Training complete!")

"""### Predict"""

print("Predicting labels for {:,} test sentences".format(len(test_input)))

model.eval()

prediction, true_labels = [], []

for batch in test_dataloader:
  batch = tuple(t.to(device) for t in batch)

  b_input_ids, b_input_mask, b_labels = batch

  with torch.no_grad():
    outputs = model(b_input_ids,
                    token_type_ids=None,
                    attention_mask=b_input_mask)
    
  logits = outputs[0]

  logits = logits.detach().cpu().numpy()
  label_ids = b_labels.to('cpu').numpy()

  prediction.append(logits)
  true_labels.append(label_ids)

print(" DONE.")

from sklearn.metrics import matthews_corrcoef

flat_prediction = [item for sublist in prediction for item in sublist]
flat_prediction = np.argmax(flat_prediction, axis=1).flatten()

flat_true_labels = [item for sublist in true_labels for item in sublist]

mcc = matthews_corrcoef(flat_true_labels, flat_prediction)

print("MCC: %.3f" %mcc)

from sklearn.metrics import accuracy_score

acc = accuracy_score(flat_true_labels, flat_prediction)

print("ACC: %.3f" %acc)

def predict(dataloader):
  prediction = []

  for batch in dataloader:
    batch = tuple(t.to(device) for t in batch)

    b_input_ids, b_input_mask = batch

    with torch.no_grad():
      outputs = model(b_input_ids,
                      token_type_ids=None,
                      attention_mask=b_input_mask)
      
    logits = outputs[0]

    logits = logits.detach().cpu().numpy()

    prediction.append(logits)
    
  flat_prediction = [item for sublist in prediction for item in sublist]
  flat_prediction = np.argmax(flat_prediction, axis=1).flatten()

  print('Prediction DONE!!!')
  return flat_prediction

df_jouska = pd.read_csv('jouska_tweet.csv')
df_anies = pd.read_csv('anies_tweet.csv')
df_terawan = pd.read_csv('terawan_tweet.csv')

def ensemble_dataloader(kalimat, max_len=128):
  MAX_LEN = max_len

  test_predict_ids = []

  for sent in kalimat:
    encoded_sent = tokenizer.encode(
        sent,
        add_special_tokens = True)
    test_predict_ids.append(encoded_sent)
  
  ################ SET PADDING #######################

  print("Max sentence length: ", max([len(sen) for sen in test_predict_ids]))

  print("Padding/truncating all sentences to %d values" % MAX_LEN)
  print('Padding token: "{:}", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))

  test_predict_ids = pad_sequences(test_predict_ids, maxlen=MAX_LEN, dtype='long', value=0, truncating='post', padding='post')
  print("Done")

  ################ SET ATTENTION MASK #######################

  attention_mask = []

  for sent in test_predict_ids:
    att_mask = [int(token_id > 0) for token_id in sent]
    attention_mask.append(att_mask)
  print('Attention Mask Finished')

  ################ ENSEMBLE DATA LOADER #######################
  test_input = torch.tensor(test_predict_ids)
  test_mask = torch.tensor(attention_mask)

  batch_size = 32
  test_data = TensorDataset(test_input, test_mask)
  test_sampler = SequentialSampler(test_data)
  test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)

   ################ OUTPUT #######################
  return test_dataloader

anies_ensemble = ensemble_dataloader(kalimat=df_anies['tweet'].values, max_len=256)
terawan_ensemble = ensemble_dataloader(kalimat=df_terawan['tweet'].values)
jouska_ensemble = ensemble_dataloader(kalimat=df_jouska['tweet'].values)

df_anies['label'] = predict(anies_ensemble)
df_terawan['label'] = predict(terawan_ensemble)
df_jouska['label'] = predict(jouska_ensemble)

df_anies['label'] = df_anies['label'].replace(2, -1)
df_terawan['label'] = df_terawan['label'].replace(2, -1)
df_jouska['label'] = df_jouska['label'].replace(2, -1)

# Checkpoint Save to CSV
df_anies.to_csv('labeling_anies.csv', index=False)
df_terawan.to_csv('labeling_terawan.csv', index=False)
df_jouska.to_csv('labeling_jouska.csv', index=False)

"""## Analyzing Output"""

n_negatif = df_anies['label'][df_anies['label'] == -1].count()
n_positif = df_anies['label'][df_anies['label'] == 1].count()
n_netral =  df_anies['label'][df_anies['label'] == 0].count()
median = np.median(df_anies['label'].values)
std_dev = np.std(df_anies['label'].values)

print(f"""
Analisis Sentimen Tweet dengan kata kunci Anies Baswedan diambil pada tanggal 27 Juli - 30 Juli, 
menghasilkan beberapa kesimpulan, yakni

Komposisi sentimen antara lain adalah 
{n_positif} tweet yang memiliki sentimen positif, 
{n_negatif} tweet dengan sentimen negatif, dan
{n_netral} tweet yang memiliki sentimen netral

Dilihat secara deskriptif statistik, nilai median sentimennya adalah {median} dan 
standar deviasinya {std_dev}

Lebih lanjut, data sentimen tersebut di visualisasikan dalam plot histogram sebagai berikut :

""")

import plotly.express as px
fig = px.histogram(df_anies, x="label")
fig.show()

n_negatif = df_terawan['label'][df_terawan['label'] == -1].count()
n_positif = df_terawan['label'][df_terawan['label'] == 1].count()
n_netral =  df_terawan['label'][df_terawan['label'] == 0].count()
median = np.median(df_terawan['label'].values)
std_dev = np.std(df_terawan['label'].values)

print(f"""
Analisis Sentimen Tweet dengan kata kunci Terawan diambil pada tanggal 27 Juli - 30 Juli, 
menghasilkan beberapa kesimpulan, yakni

Komposisi sentimen antara lain adalah 
{n_positif} tweet yang memiliki sentimen positif, 
{n_negatif} tweet dengan sentimen negatif, dan
{n_netral} tweet yang memiliki sentimen netral

Dilihat secara deskriptif statistik, nilai median sentimennya adalah {median} dan 
standar deviasinya {std_dev}

Lebih lanjut, data sentimen tersebut di visualisasikan dalam plot histogram sebagai berikut :

""")

fig = px.histogram(df_terawan, x="label")
fig.show()

n_negatif = df_jouska['label'][df_jouska['label'] == -1].count()
n_positif = df_jouska['label'][df_jouska['label'] == 1].count()
n_netral =  df_jouska['label'][df_jouska['label'] == 0].count()
median = np.median(df_jouska['label'].values)
std_dev = np.std(df_jouska['label'].values)

print(f"""
Analisis Sentimen Tweet dengan kata kunci Jouska diambil pada tanggal 27 Juli - 30 Juli, 
menghasilkan beberapa kesimpulan, yakni

Komposisi sentimen antara lain adalah 
{n_positif} tweet yang memiliki sentimen positif, 
{n_negatif} tweet dengan sentimen negatif, dan
{n_netral} tweet yang memiliki sentimen netral

Dilihat secara deskriptif statistik, nilai median sentimennya adalah {median} dan 
standar deviasinya {std_dev}

Lebih lanjut, data sentimen tersebut di visualisasikan dalam plot histogram sebagai berikut :

""")

fig = px.histogram(df_jouska, x="label")
fig.show()